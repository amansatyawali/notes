https://drive.google.com/drive/folders/1PEalHsRN9n1zQ7v_UF2OOlCzKm5a91Yo

Features of ou language :

Sentences are made of words, words are part of letters

Grammar

Tone

Expressions

Phonetics

Variety - different ways of writing same words

Ambiguity - one word havung multiple meanings


Why NLP?

Most data in our civilizations is unstructured.


Applications of NLP :

Chatbots

Text prediction

Translation

Text summarization

Sentiment classification

Search







HuggingFace


  Corpus -> Collection of text data

  Token -> Words(or symbols) which cannot be reduced



Preprocessing :

Removing punctuations, accent marks and other diacritics

removing white spaces

  Although removing white spaces helps to some extent, but sometimes it hurts to split only by white space, 
  for eg : 'New York', 'Rock 'n' Roll', I'm

Expanding abbreviations

removing stop words, sparse terms and particular words

Text 

converting to lower case

converting numbers into words or removing numbers 


Stemming : 

  The process of converting a word to its stem form by removing morphological additions from a word
  but it also makes a lot of mistakes :
  player, played -> played

  tradition -> tradit

  denied -> deni 


  But the reason it is still considered sometimes is because it is very fast

  Mostly used for information retrieval and classification

Lemmatisation :
 
  This process also finds the root of a word, but it does that by using a dictionary and analyses the part of the speech 
  to understand the actual root of that word.



Bag of words technique :

  create a table from the data where each column represents a word in the corpus and every sentence is a row
  and each cell has the frequency of those words in that row(sentence)

  Since each sentence mostly contains less than 98% of all the unique words in the corpus, most of that table is empty, 
  hence it is called sparse data.

  It does not capture the meaning of the text, but is able to do minor classifications.

  Also, the size of the columns becomes very high, because of which, the table gets very big


Bigrams and N-grams :

  We try to find phrases that have occurred in the corpus
  This is done after tokenization and lemmatization
  so if we are finding bigrams, we will look for 2 words that have occurred together.

  Example : 'I have really enjoyed reading this book'
    The bigrams will be : [[I, have], [have, really], [really, enjoyed], [enjoyed, reading], [reading, this], [this, book]]

  For N-grams, we will take N tokens at a time

  In case of bigrams, the number of columns will be more than in case of BOW

  For BOWs or 1-grams, the space is of the order n

  In case of bigrams, it will be less than the order of n^2

  so in case of N-grams, it will be less than the order of n^N

  Bigrams give a good idea of the context. if we take trigrams, it might give us event better idea of the context
  but as we keep on increasing N, the N-grams become too specific and might not occur a lot of times in the corpus and might
  not occur even once in testing or production. Also, the space it requires increases exponentially and the table will become 
  more and more sparse as we increase N.
  Therefore, mostly only values upto 3 are used in N-grams.


TF-IDF :

  Term frequency - Inverse document frequency tells us how important a particular word is to a document in a corpus.

  Term frequency - (Number of times a term t appears in a document)*(Total number of words in a document)

  This is calculated for each term in each document

  *Read max_df and min_df

  Max_df value ranges from 0-1, It gives a threshold of document frequency, over which a word would be ignored.
  by default it is 1, that means all the words in all documents will be counted
  If the value is 0.9, that means that the words with document frequency above 90% will be ignored and will not be 
  used as tokens while creating the table

  max_features :
  
  The limit of tokens to be taken, if we take it as 10000, the top 10000 tokens with the highest TF in the corpus will be taken
